<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<!-- saved from url=(0031)http://www.cs.cmu.edu/~lerrelp/ -->
<html class="gr__cs_cmu_edu"><head><meta http-equiv="Content-Type" content="text/html; charset=windows-1252">
        
        <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
        <meta name="keywords" content="Lerrel, Pinto, Robot, Learning, Computer Vision, Big Data">
        <meta name="description" content="About Lerrel Pinto">
        <meta name="author" content="Lerrel Pinto">
        <meta name="robots" content="index">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <style type="text/css">
            /* Stolen from Jon Barron via Aayush */
            a {
            color: #1772d0;
            text-decoration:none;
            }
            a:focus, a:hover {
            color: #f09228;
            text-decoration:none;
            }
            body,td,th {
            font-family: 'Lato', Verdana, Helvetica, sans-serif;
            font-size: 14px
            }
            strong {
            font-family: 'Lato', Verdana, Helvetica, sans-serif;
            font-size: 13px
            }
            heading {
            font-family: 'Lato', Verdana, Helvetica, sans-serif;
            font-size: 15px;
            font-weight: 700
            }
        </style>
        <link rel="icon" type="image/png" href="http://www.ri.cmu.edu/images/site_images/favicon.png">
        <title>Lekha Mohan</title>
        <link href="./files/css" rel="stylesheet" type="text/css">
    </head>
    <body data-gr-c-s-loaded="true">
        <table width="900" border="0" align="center" cellspacing="0" cellpadding="0">
        <tbody>
            <tr>
                <td>
                    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                        <tbody>
                            <tr>
                                <td width="67%" valign="middle">
                                    <p align="center"><font size="6">Lekha Mohan</font>
                                    </p>
                                    <div align="justify">
                                        <p>I am Lekha Mohan, a research assistant at <a href="http://www.ri.cmu.edu/">The Robotics Institute</a> <a href="http://www.cmu.edu/">Carnegie Mellon University</a> under <a href="http://www.cs.cmu.edu/~abhinavg/">Abhinav Gupta</a>. I graduated from M.S in Robotic systems Development from RI in 2016 where I worked with several robot platforms to enhance complex robot manipulation.                                         </p>
                                        <p>My research interests revolve around leveraging complex robot manipulation behavior and harnessing useful robot databases. I am excited about enabling robots to perform complex and comprehensive manipulation skills. I have worked on sub-modules of grasping such as perception, grasping and motion planning. My current research project focuses on <i>Learning from Human Demonstrations using Baxter Robot Platform</i>

                                        </p>
                                    </div>
                                    <p align="center">
                                        <style type="”text/css”">
                                            p span.displaynone { display:none; }
                                        </style>
				    </p><p>You can view my CV <a href="./files/CV.pdf">here</a>	
                                    </p>Email:lekhamohan[AT]<span class="”displaynone”"></span>cmu.edu</p>
                                </td>
                                <td width="33%"><img width="300"  src="./files/display_pic.JPG"></td>
                            </tr>
                        </tbody>
                    </table>
                    
                    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                        <tbody>
                            <tr>
                                <td>
                                </td>
                            </tr>
                        </tbody>
                    </table>
                    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                        <tbody>
                            <tr>
                                <td width="100%"> 
                                      <iframe width="560" height="315" src="https://www.youtube.com/embed/dO9UIUu0-fY" frameborder="0" gesture="media" allow="encrypted-media" allowfullscreen></iframe>
                                 </td>
                            </tr>
                        </tbody>
                    </table>
                    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                        <tbody>
                            <tr>
                                <td>
                                    <heading>Research and Selected Projects</heading>
                                </td>
                            </tr>
                        </tbody>
                    </table>

                    <!-- IMITATION Learning -->
                    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                        <tbody>
                            <tr>
                               <td width="25%" valign="top">
					<div style="float: left; width: 200px; overlfow: hidden">
                                	<a href="https://youtu.be/oM_NsD_hhr0"><img class="centered-and-cropped" src="https://j.gifs.com/6RYymz.gif" width="150" height="120" alt="GIF"></a>
                                	<div style="float: right; width: 200px; overflow: hidden">
		    </br>			
					<a href="https://youtu.be/8per-QCfqG8"><img class="centered-and-cropped" src="https://j.gifs.com/BL1p1o.gif" width="150" height="120" alt="GIF"> </a>
				</td> 
                                <td width="75%" valign="top">
                                    <p>
                                        <a href="https://www.youtube.com/watch?v=0kE4S_3Jsmw">
                                            <heading>Learning from Human Demonstrations</heading>
                                        </a>
                                        <!--</a>--><br>
                                        <strong>Lekha</strong>, <a href="http://www.cs.cmu.edu/~lerrelp/">Lerrel Pinto</a> , <a href="http://www.cs.cmu.edu/~abhinavg/">Abhinav Gupta</a><br>
                                    </p><p>
                                    Can Baxter perform complex manipulation actions by looking at your demo video? I am currently working on imitation learning by the Baxter robot focusing on leveragin complex robot manipulation. ImageNet had immensely accelerated the growth of computer vision models. Inspired the research impact of ImageNet and to address lack of sufficient robot data, I am developing a novel large-scale dataset of 13,000 human-robot demonstrations for 20 classes of diverse manipulation actions. These actions include opening bottles, passing objects from one hand to another, stacking etc., using objects of various shapes and sizes. I record human demonstrations and the kinesthetic imitation of the same task on the robot. Using this large-scale data, I am trying to jointly-learn correspondences between the human actions and robot actions in order to develop robust imitation learning frameworks and improve generalization across unseen tasks. We will be releasing this dataset for the benefit of other researchers.
                                    </p>
				    <br><br>
                                </td>
                            </tr>
                        </tbody>

                        <!-- Human Assistive Robotic Picker-UR5 Platform - SBPL -->
						<tbody>
                        <tr>
                            <td width="25%">
                                <a href="https://www.youtube.com/watch?v=ko8Cr8kBc-Q"><img class="centered-and-cropped" src="https://j.gifs.com/VP5DAW.gif" width="200" height="150" alt="GIF"></a>
                            </td>    
                            <td width="75%" valign="top">
                                    <p>
                                        <a href="https://www.youtube.com/watch?v=ko8Cr8kBc-Q">
                                            <heading>Human Assistive Robotic Picker - UR5 Platform</heading>
                                        </a>
                                        <!--</a>--><br>
                                        <a href="https://www.youtube.com/watch?v=ko8Cr8kBc-Q">video</a>
                                        <strong>Lekha Mohan</strong>, Alex Brinkman, Rick Shanor, Abhishek Bhatia and Feroze Naina<br>
                                    </p><p>Amazon has automated their warehouses by using robots to move storage shelves. However, they still require human intervention to pick each object from the shelf bin and place it into the shipping box. Our primary goal was to solve this problem by developing a robot system that can automatically parse a list of items, identify desired items on a shelf, and pick and place them into the order bin. We collaborated  with <a href="http://www.cs.cmu.edu/~maxim/">Maxim Likhachev</a> and the Search Based Planning Lab to compete in the 2016 <strong>Amazon Picking Challenge</strong>, Leipzig. Our system, the Human Assistive Robotic Picker (HARP), consists of perception, grasping and planning sub systems. The UR5 robot platform, outfitted with a suction gripper, picks up small household objects from the twelve shelf bins. <!-- We have validated the individual subsystems by achieving desired perception accuracies and demonstrated UR5 pick and place task. -->
.</p>
                                        <br><br>
                                </p></td>
                            </tr>
                        </tbody>

                        <!-- HERB -->
                        <tbody>
                        <tr>
                            <td width="25%">
                                <a href="https://www.youtube.com/watch?v=YMH-w38l64s&feature=youtu.be"><img class="centered-and-cropped" src="https://j.gifs.com/BL8MQn.gif" width="200" height="150" alt="GIF"></a>
                            </td> 
                            <td width="75%" valign="top">
                                <p>
                                    <a href="https://www.youtube.com/watch?v=YMH-w38l64s&feature=youtu.be">
                                        <heading>Table Clearing Task by HERB Platform</heading>
                                    </a>
                                    <!--</a>--><br>
                                    <strong>Lekha Mohan</strong>, Gauri gandhi, Rohith Dasarathi, Keerthana Manivannan<br>
                                </p><p>
            Robots performing household tasks is a big challenge, where the operator/user needs to give sequential commands to the robot for each subtask. In order to perform a complicated task in say,a kitchen environment, task planners are needed that can reason over very large sets of states by manipulating partial descriptions, while geometric planners operate on completely detailed geometric specifications of world states. Home Exploring Robot Butler (HERB) currently uses a sequential task planner for table clearing which plans for the entire task and obtains a feasible global plan before beginning execution. We implemented a task planner that reduces the planning time by breaking the high level task into subgoals, making choices and committing to them, greatly reducing the length of plans. This approach is suitable for complex tasks where strict optimality is not crucial. This framework is Hierarchical Planning and its advantages led us to use these key features (fluents, operators, etc.) in a sub-version that uses a breadth-first approach to plan in the now.
 
            </p>
                <br><br>
                            </td>
                        </tr>
                    </tbody>


                        <!-- commentt -->
						<tbody>
                            <tr>
                               <td width="25%">
                                                            </td> 
                                <td width="75%" valign="top">
                                    <p>
                                        <a href="https://drive.google.com/file/d/0B1Jn7RtPbkDROEZIdklsRHFsc1E/view?usp=sharing">
                                            <heading>Visualization tools for predicting robots trajectory</heading>
                                        </a>
                                        <!--</a>--><br>
                                        Robotics Intern'16 at <a href="http://5drobotics.com/">5D Robotics</a><br>
                                    </p> 
                                    <p>During this summer internship, I developed algorithms to predict the ground robot's trajectory inorder to prevent potential accidental collisions due to latency caused by in-house custom sensors. I integrated my algorithm in a RViz plugin which was deployed at one of the 5D Robotics's industrial facility. 
                                    </p>

                                </td>
                            </tr>
                        </tbody>

                        <!-- Independent project -->
						<tbody>
                            <tr>
                               <td width="25%">
                                <a href="https://www.youtube.com/watch?v=Ffyi9W23Fl0"><img class="centered-and-cropped" src="https://j.gifs.com/jqvwn5.gif" width="200" height="150" alt="GIF"></a>
                            </td> 
                                <td width="75%" valign="top">
                                    <p>
                                        <a href="https://www.youtube.com/watch?v=Ffyi9W23Fl0">
                                            <heading>Direct perception based automation of car in racing game</heading>
                                        </a>
                                        <!--</a>--><br>
                                        <a href="https://saiprabhakar.github.io/files/deep_driving.pdf">pdf</a> , <a href="https://github.com/saiprabhakar/DeepDriving">code</a> 
                                        
                                    </p> 
                                    <p> <!-- There are two major paradigms for vision-based autonomous driving systems: mediated perception approaches that parse an entire scene to make a driving decision, and behavior reflex approaches that directly map an input image to a driving action by a regressor. A third paradigm: a --> In vision based autonomous systems, direct perception based approach that is used to estimate the affordance for driving was proposed <a href="http://deepdriving.cs.princeton.edu/paper.pdf">recently</a>. Here the input image is mapped to a small number of key perception indicators that directly relate to the affordance of a road/traffic state for driving. Our project uses AlexNet training from scratch for this mapping. We explored the possibility using transfer learning for learning the mapping. We used pre-trained Alexnet to map the input image to a set of key perception indicators. These indicators enables a simple controller to drive the car autonomously. We fine-tuned weights for all the convolution layers (layers 1-5) in the network and trained from scratch for all the fully connected layers (layers 6-8) to preserve the aspect ratio of the images obtained from the simulator.
</p>

                                </td>
                            </tr>
                        </tbody>

                        </table><table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                            <tbody>
                                <tr>
                                    <td>
                                        <br>
                                    </td>
                                </tr>
                            </tbody>
                        </table>
                        </td>
                        </tr>
                        </tbody>
                    </table>
    
<a href="https://people.eecs.berkeley.edu/~barron/">Source stolen from here</a>

</body></html>
