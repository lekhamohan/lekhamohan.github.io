<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<!-- saved from url=(0031)http://www.cs.cmu.edu/~lerrelp/ -->
<html class="gr__cs_cmu_edu"><head><meta http-equiv="Content-Type" content="text/html; charset=windows-1252">
        
        <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
        <meta name="keywords" content="Lerrel, Pinto, Robot, Learning, Computer Vision, Big Data">
        <meta name="description" content="About Lerrel Pinto">
        <meta name="author" content="Lerrel Pinto">
        <meta name="robots" content="index">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <style type="text/css">
            /* Stolen from Jon Barron via Aayush */
            a {
            color: #1772d0;
            text-decoration:none;
            }
            a:focus, a:hover {
            color: #f09228;
            text-decoration:none;
            }
            body,td,th {
            font-family: 'Lato', Verdana, Helvetica, sans-serif;
            font-size: 14px
            }
            strong {
            font-family: 'Lato', Verdana, Helvetica, sans-serif;
            font-size: 13px
            }
            heading {
            font-family: 'Lato', Verdana, Helvetica, sans-serif;
            font-size: 15px;
            font-weight: 700
            }
        </style>
        <link rel="icon" type="image/png" href="http://www.ri.cmu.edu/images/site_images/favicon.png">
        <title>Lekha Mohan</title>
        <link href="./files/css" rel="stylesheet" type="text/css">
    </head>
    <body data-gr-c-s-loaded="true">
        <table width="900" border="0" align="center" cellspacing="0" cellpadding="0">
        <tbody>
            <tr>
                <td>
                    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                        <tbody>
                            <tr>
                                <td width="67%" valign="middle">
                                    <p align="center"><font size="6">Lekha Mohan</font>
                                    </p>
                                    <div align="justify">
                                        <p>I am Lekha Mohan, a research assistant at <a href="http://www.ri.cmu.edu/">The Robotics Institute</a> <a href="http://www.cmu.edu/">Carnegie Mellon University</a> under <a href="http://www.cs.cmu.edu/~abhinavg/">Abhinav Gupta</a>. I graduated from M.S in Robotic systems Development from RI in 2016 where I worked with several robot platforms to enhance complex robot manipulation.                                        </p>
                                        <p>My research interests revolve around leveraging complex robot manipulation behavior and harnessing useful robot databases. I am excited about making robots perform complex and comprehensive manipulation skills. I have worked on sub-modules of grasping such as perception, grasping and motion planning. My current research project focuses on <i>Learning from Human Demonstrations using Baxter Robot Platform</i>

                                        </p>
                                    </div>
                                    <p align="center">
                                        <style type="”text/css”">
                                            p span.displaynone { display:none; }
                                        </style>
                                    </p><p align="center">Email:lekhamohan[AT]<span class="”displaynone”"></span>cmu.edu</p>
				    <p></p>
                                    <!--. <a href="./CV.pdf">CV</a>--> 
                                    <p></p>
                                </td>
                                <td width="33%"><img width="300"  src="./files/display_pic.JPG"></td>
                            </tr>
                        </tbody>
                    </table>
                    <!--
                    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                        <tbody>
                            <tr>
                                <td>
                                    <heading>Press Coverage</heading>
                                </td>
                            </tr>
                        </tbody>
                    </table>
                    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                        <tbody>
                            <tr>
                                <td width="100%">
                                    <p>Learning to Fly by Crashing</p>
                                        <a href="http://spectrum.ieee.org/automaton/robotics/drones/drone-uses-ai-and-11500-crashes-to-learn-how-to-fly"><img src="https://fs25.formsite.com/ieeevcep/images/IEEE-Spectrum.Horizontal.jpg" height="60" width="180" style="border-style: none"></a>
                                        <a href="http://www.popularmechanics.com/flight/drones/a26490/drones-learned-to-fly-by-crashing/"><img src="http://pop.h-cdn.co/assets/popularmechanics/20170509153642/images/apple-touch-icon.png" height="60" style="border-style: none"></a>
                                        <a href="https://www.digitaltrends.com/cool-tech/crashing-drones-teach-fly-better/"><img src="https://online-writing-jobs.com/wp-content/uploads/writejob-digitaltrends.jpg" height="60" style="border-style: none"></a>
                                   <p>Curious Robot</p>
				                        <a href="https://techcrunch.com/2017/06/28/teaching-robots-to-learn-about-the-world-through-touch/?ncid=mobilerecirc_recent"><img src="https://s0.wp.com/wp-content/themes/vip/techcrunch-2013/assets/images/logo.svg" height="60" style="border-style: none"></a>
                                        <a href="http://qz.com/675276/ais-are-starting-to-learn-like-human-babies-by-grasping-and-poking-objects/"><img src="https://app.qz.com/img/qz_og_img.png" height="60" style="border-style: none"></a>
                                        <a href="http://www.sbs.com.au/topics/science/future/article/2016/05/09/ais-are-starting-learn-human-babies-grasping-and-poking-objects"><img src="http://d39fx46bzv2q62.cloudfront.net/wp-content/uploads/2012/06/sbs2.jpg" alt="SBS Review" height="60" style="border-style: none"></a>
                                 </td>
                            </tr>
                        </tbody>
                    </table>-->
                    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                        <tbody>
                            <tr>
                                <td>
                                    <heading>Research and Selected Projects</heading>
                                </td>
                            </tr>
                        </tbody>
                    </table>

                    <!-- IMITATION Learning -->
                    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                        <tbody>
                            <tr>
                               <td width="25%">
                                <a href="https://www.youtube.com/watch?v=0kE4S_3Jsmw"><img class="centered-and-cropped" src="https://j.gifs.com/BL8MOn.gif" width="200" height="150" alt="GIF"></a>
                            </td> 
                                <td width="75%" valign="top">
                                    <p>
                                        <a href="https://www.youtube.com/watch?v=0kE4S_3Jsmw">
                                            <heading>Learning from Human Demonstrations</heading>
                                        </a>
                                        <!--</a>--><br>
                                        <strong>Lekha</strong>, <a href="http://www.cs.cmu.edu/~lerrelp/">Lerrel Pinto</a> , <a href="http://www.cs.cmu.edu/~abhinavg/">Abhinav Gupta</a><br>
                                    </p><p>
                                    Can Baxter perform complex manipulation actions by looking at your demo video? I am currently working on imitation learning by the Baxter robot. I am developing a novel large-scale dataset for robot imitation learning. 13,000 human-robot demonstrations for 20 classes of diverse manipulation actions. I record human demonstration and kinesthetic imitation kinesthetic imitation of the same task using the robot. Using this large-scale data, I intend to jointly-learn correspondences between the human actions and robot actions. We will also be releasing this dataset for other researchers.
                                    </p>
				    <br><br>
                                </td>
                            </tr>
                        </tbody>

                        <!-- Human Assistance Robotics Picker - UR5 Platform -->
						<tbody>
                        <tr>
                            <td width="25%">
                                <a href="https://www.youtube.com/watch?v=ko8Cr8kBc-Q"><img class="centered-and-cropped" src="https://j.gifs.com/VP5DAW.gif" width="200" height="150" alt="GIF"></a>
                            </td>    
                            <td width="75%" valign="top">
                                    <p>
                                        <a href="https://www.youtube.com/watch?v=ko8Cr8kBc-Q">
                                            <heading>Human Assistive Robotic Picker - UR5 Platform</heading>
                                        </a>
                                        <!--</a>--><br>
                                        <a href="https://www.youtube.com/watch?v=ko8Cr8kBc-Q">video</a>
                                        Alex Brinkman, <strong>Lekha Mohan</strong>, Rick Shanor, Abhishek Bhatia and Feroze Naina<br>
                                    </p><p>Amazon has automated their warehouses by using robots to move storage shelves. However, they still require human intervention to pick each object from the shelf bin and place it into the shipping box. Our primary goal was to solve this problem by developing a robot system that can automatically parse a list of items, identify desired items on a shelf, and pick and place them into the order bin. We partnered with <a href="http://www.cs.cmu.edu/~maxim/">Maxim Likhachev</a> and the Search Based Planning Lab to compete in the 2016 Amazon Picking Challenge, Leipzig. Our system, the Human Assistive Robotic Picker (HARP), consists of perception, gripping and platform sub systems. The perception system identifies items of interest based on their known geometric models. The UR5 robot platform, outfitted with a suction gripper, picks up small household objects from the twelve shelf bins. We have validated the individual subsystems by achieving desired perception accuracies and demonstrating UR5 pick and place task planning in simulation.
.</p>
                                        <br><br>
                                </p></td>
                            </tr>
                        </tbody>

                        <!-- curriculum learning -->
                        <tbody>
                        <tr>
                            <td width="25%">
                                <a href="https://www.youtube.com/watch?v=YMH-w38l64s&feature=youtu.be"><img class="centered-and-cropped" src="https://j.gifs.com/BL8MQn.gif" width="200" height="150" alt="GIF"></a>
                            </td> 
                            <td width="75%" valign="top">
                                <p>
                                    <a href="https://www.youtube.com/watch?v=YMH-w38l64s&feature=youtu.be">
                                        <heading>Table Clearing Task by HERB Platform</heading>
                                    </a>
                                    <!--</a>--><br>
                                    Gauri gandhi, <strong>Lekha Mohan</strong>, Keerthana, Jimit Gandhi<br>
                                </p><p>
            Robots performing household tasks is a big challenge, where the operator/user needs to give sequential commands to the robot for each subtask. In order to perform a complicated task in say,a kitchen environment, task planners are needed that can reason over very large sets of states by manipulating partial descriptions, while geometric planners operate on completely detailed geometric specifications of world states. Home Exploring Robot Butler (HERB) currently uses a sequential task planner for table clearing which plans for the entire task and obtains a feasible global plan before beginning execution. We implemented a task planner that reduces the planning time by breaking the high level task into subgoals, making choices and committing to them, greatly reducing the length of plans. This approach is suitable for complex tasks where strict optimality is not crucial. This framework is Hierarchical Planning and its advantages led us to use these key features (fluents, operators, etc.) in a sub-version that uses a breadth-first approach to plan in the now.
 
            </p>
                <br><br>
                            </td>
                        </tr>
                    </tbody>


                        <!-- Legged and wheeled robot -->
						<tbody>
                            <tr>
                               <td width="25%">
                                                            </td> 
                                <td width="75%" valign="top">
                                    <p>
                                        <a href="https://drive.google.com/file/d/0B1Jn7RtPbkDROEZIdklsRHFsc1E/view?usp=sharing">
                                            <heading>Visualization tools for predicting robots trajectory</heading>
                                        </a>
                                        <!--</a>--><br>
                                        Robotics Intern at <a href="http://5drobotics.com/">5D Robotics</a><br>
                                    </p> 
                                    <p>My work consisted of developing algorithms to predict robots trajectory and prevent accidental collisions if any due to latency caused by custom sensors. I integrated my algorithm developed on ROS with a RViz plugin which was deployed at one of 5D Robotics customer industry.
                                    </p>

                                </td>
                            </tr>
                        </tbody>

                        <!-- Systemantics Intern -->
						<tbody>
                            <tr>
                               <td width="25%">
                                <a href="https://www.youtube.com/watch?v=Ffyi9W23Fl0"><img class="centered-and-cropped" src="https://j.gifs.com/jqvwn5.gif" width="200" height="150" alt="GIF"></a>
                            </td> 
                                <td width="75%" valign="top">
                                    <p>
                                        <a href="https://www.youtube.com/watch?v=Ffyi9W23Fl0">
                                            <heading>Direct perception based automation of car in racing game</heading>
                                        </a>
                                        <!--</a>--><br>
                                        <a href="https://saiprabhakar.github.io/files/deep_driving.pdf">pdf</a> , <a href="https://github.com/saiprabhakar/DeepDriving">code</a> 
                                        
                                    </p> 
                                    <p>Today, there are two major paradigms for vision-based autonomous driving systems: mediated perception approaches that parse an entire scene to make a driving decision, and behavior reflex approaches that directly map an input image to a driving action by a regressor. A third paradigm: a direct perception based approach to estimate the affordance for driving was proposed <a href="http://deepdriving.cs.princeton.edu/paper.pdf">recently</a>. Where the input image is mapped to a small number of key perception indicators that directly relate to the affordance of a road/traffic state for driving. The work uses AlexNet training from scratch for the mapping. This worked explored the possibility using transfer learning for learning the mapping. We used pre-trained Alexnet to map the input image to a set of key perception indicators. These indicators enables a simple controller to drive the car autonomously. We fine-tuned weights for all the convolution layers (layers 1-5) in the network and trained from scratch for all the fully connected layers (layers 6-8) to preserve the aspect ratio of the images obtained from the simulator.
</p>

                                </td>
                            </tr>
                        </tbody>

                        </table><table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                            <tbody>
                                <tr>
                                    <td>
                                        <br>
                                    </td>
                                </tr>
                            </tbody>
                        </table>
                        </td>
                        </tr>
                        </tbody>
                    </table>
    
<a href="https://people.eecs.berkeley.edu/~barron/">Source stolen from here</a>

</body></html>
